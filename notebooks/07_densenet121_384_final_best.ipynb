{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Final Model Comparison – DenseNet121 Knee OA KL Grading\n",
        "This notebook compares the 320 baseline and the improved 384 final model."
      ],
      "metadata": {
        "id": "7pBKhl8GTeOP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2TlR0mFIil0",
        "outputId": "1a474156-c09a-4b8a-a6d4-80d56c75768b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Dataset folder already exists, skipping unzip.\n",
            "✅ Dataset ready\n",
            "Content of /content: ['.config', 'knee-osteoarthritis-dataset-with-severity', 'best_densenet121_320_tricks.pt', 'drive', 'sample_data']\n",
            "TRAIN_DIR: /content/knee-osteoarthritis-dataset-with-severity/train\n",
            "VAL_DIR: /content/knee-osteoarthritis-dataset-with-severity/val\n",
            "TEST_DIR: /content/knee-osteoarthritis-dataset-with-severity/test\n",
            "Train size: 5778 Val size: 826 Test size: 1656\n",
            "Classes: ['0', '1', '2', '3', '4']\n",
            "Train class counts: [2286 1046 1516  757  173]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-777/1289732258.py:268: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(USE_AMP and device.type == \"cuda\"))\n",
            "/tmp/ipython-input-777/1289732258.py:321: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
            "/tmp/ipython-input-777/1289732258.py:283: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01/15 | train_loss=1.2257 | val_loss=1.5756 | val_acc=0.1961 | val_macroF1=0.0849 | lr=1.00e-04 | time=69.8s\n",
            "  ✅ Saved best checkpoint: best_densenet121_320_tricks.pt (val_macroF1=0.0849)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-777/1289732258.py:321: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
            "/tmp/ipython-input-777/1289732258.py:283: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 02/15 | train_loss=1.0074 | val_loss=1.5009 | val_acc=0.4540 | val_macroF1=0.3096 | lr=1.00e-04 | time=68.7s\n",
            "  ✅ Saved best checkpoint: best_densenet121_320_tricks.pt (val_macroF1=0.3096)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-777/1289732258.py:321: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
            "/tmp/ipython-input-777/1289732258.py:283: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 03/15 | train_loss=0.9287 | val_loss=1.4435 | val_acc=0.4915 | val_macroF1=0.3330 | lr=9.85e-05 | time=68.4s\n",
            "  ✅ Saved best checkpoint: best_densenet121_320_tricks.pt (val_macroF1=0.3330)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-777/1289732258.py:321: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
            "/tmp/ipython-input-777/1289732258.py:283: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 04/15 | train_loss=0.8716 | val_loss=1.2669 | val_acc=0.5690 | val_macroF1=0.4391 | lr=9.43e-05 | time=69.7s\n",
            "  ✅ Saved best checkpoint: best_densenet121_320_tricks.pt (val_macroF1=0.4391)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-777/1289732258.py:321: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
            "/tmp/ipython-input-777/1289732258.py:283: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 05/15 | train_loss=0.8673 | val_loss=1.0549 | val_acc=0.5400 | val_macroF1=0.6200 | lr=8.74e-05 | time=69.1s\n",
            "  ✅ Saved best checkpoint: best_densenet121_320_tricks.pt (val_macroF1=0.6200)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-777/1289732258.py:321: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
            "/tmp/ipython-input-777/1289732258.py:283: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 06/15 | train_loss=0.8382 | val_loss=0.9809 | val_acc=0.6017 | val_macroF1=0.6080 | lr=7.84e-05 | time=69.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-777/1289732258.py:321: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
            "/tmp/ipython-input-777/1289732258.py:283: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 07/15 | train_loss=0.7874 | val_loss=0.9490 | val_acc=0.5860 | val_macroF1=0.6196 | lr=6.77e-05 | time=68.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-777/1289732258.py:321: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
            "/tmp/ipython-input-777/1289732258.py:283: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 08/15 | train_loss=0.7452 | val_loss=0.9197 | val_acc=0.5751 | val_macroF1=0.6405 | lr=5.60e-05 | time=69.1s\n",
            "  ✅ Saved best checkpoint: best_densenet121_320_tricks.pt (val_macroF1=0.6405)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-777/1289732258.py:321: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
            "/tmp/ipython-input-777/1289732258.py:283: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 09/15 | train_loss=0.7004 | val_loss=0.9243 | val_acc=0.5847 | val_macroF1=0.6548 | lr=4.40e-05 | time=69.2s\n",
            "  ✅ Saved best checkpoint: best_densenet121_320_tricks.pt (val_macroF1=0.6548)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-777/1289732258.py:321: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
            "/tmp/ipython-input-777/1289732258.py:283: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/15 | train_loss=0.6687 | val_loss=0.8673 | val_acc=0.6053 | val_macroF1=0.6761 | lr=3.23e-05 | time=69.6s\n",
            "  ✅ Saved best checkpoint: best_densenet121_320_tricks.pt (val_macroF1=0.6761)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-777/1289732258.py:321: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
            "/tmp/ipython-input-777/1289732258.py:283: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/15 | train_loss=0.6026 | val_loss=0.8749 | val_acc=0.6114 | val_macroF1=0.6790 | lr=2.16e-05 | time=69.8s\n",
            "  ✅ Saved best checkpoint: best_densenet121_320_tricks.pt (val_macroF1=0.6790)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-777/1289732258.py:321: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
            "/tmp/ipython-input-777/1289732258.py:283: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/15 | train_loss=0.5973 | val_loss=0.8310 | val_acc=0.6368 | val_macroF1=0.6938 | lr=1.26e-05 | time=69.1s\n",
            "  ✅ Saved best checkpoint: best_densenet121_320_tricks.pt (val_macroF1=0.6938)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-777/1289732258.py:321: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
            "/tmp/ipython-input-777/1289732258.py:283: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/15 | train_loss=0.5854 | val_loss=0.8337 | val_acc=0.6683 | val_macroF1=0.7065 | lr=5.73e-06 | time=68.6s\n",
            "  ✅ Saved best checkpoint: best_densenet121_320_tricks.pt (val_macroF1=0.7065)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-777/1289732258.py:321: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
            "/tmp/ipython-input-777/1289732258.py:283: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/15 | train_loss=0.5841 | val_loss=0.8591 | val_acc=0.6344 | val_macroF1=0.6968 | lr=1.45e-06 | time=69.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-777/1289732258.py:321: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
            "/tmp/ipython-input-777/1289732258.py:283: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/15 | train_loss=0.5359 | val_loss=0.8506 | val_acc=0.6586 | val_macroF1=0.7076 | lr=0.00e+00 | time=69.3s\n",
            "  ✅ Saved best checkpoint: best_densenet121_320_tricks.pt (val_macroF1=0.7076)\n",
            "\n",
            "✅ Best Val Macro-F1: 0.7075860183108729\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-777/1289732258.py:283: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== TEST RESULTS (Best EMA) =====\n",
            "Test Loss:     0.7971\n",
            "Test Accuracy: 0.6739\n",
            "Test Macro-F1: 0.6914\n",
            "\n",
            "Confusion Matrix:\n",
            " [[503 125  10   1   0]\n",
            " [111 138  43   4   0]\n",
            " [ 27 135 251  34   0]\n",
            " [  1  10  22 184   6]\n",
            " [  0   0   0  11  40]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7835    0.7872    0.7853       639\n",
            "           1     0.3382    0.4662    0.3920       296\n",
            "           2     0.7699    0.5615    0.6494       447\n",
            "           3     0.7863    0.8251    0.8053       223\n",
            "           4     0.8696    0.7843    0.8247        51\n",
            "\n",
            "    accuracy                         0.6739      1656\n",
            "   macro avg     0.7095    0.6849    0.6914      1656\n",
            "weighted avg     0.7033    0.6739    0.6822      1656\n",
            "\n",
            "\n",
            "Saved: best_densenet121_320_tricks_state_dict.pt\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# AUTO DATASET LOADER (Drive ZIP) + DenseNet121@320 Tricks\n",
        "# - Drive mount + unzip\n",
        "# - auto-detect train/val/test\n",
        "# - WeightedRandomSampler\n",
        "# - MixUp\n",
        "# - EMA\n",
        "# - Cosine LR + warmup\n",
        "# - AMP\n",
        "# ============================================\n",
        "\n",
        "# ----------------------------\n",
        "# 0) Imports\n",
        "# ----------------------------\n",
        "import os, math, time, copy, random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "from sklearn.metrics import f1_score, confusion_matrix, classification_report, accuracy_score\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Reproducibility + Device\n",
        "# ----------------------------\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "\n",
        "seed_everything(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ==========================================================\n",
        "# 2) DATASET LOADING FROM GOOGLE DRIVE ZIP\n",
        "# ==========================================================\n",
        "DATASET_NAME = \"knee-osteoarthritis-dataset-with-severity\"  # folder name after unzip (can be different)\n",
        "ZIP_NAME = \"knee_oa_dataset.zip\"  # <-- CHANGE to your zip name in MyDrive\n",
        "\n",
        "# Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "zip_path = f\"/content/drive/MyDrive/{ZIP_NAME}\"\n",
        "if not os.path.isfile(zip_path):\n",
        "    raise FileNotFoundError(f\"ZIP file not found at: {zip_path}\\n\"\n",
        "                            f\"➡️ Upload your dataset zip to Google Drive > MyDrive and set ZIP_NAME correctly.\")\n",
        "\n",
        "# Unzip if needed\n",
        "extract_root = \"/content\"\n",
        "dataset_root = os.path.join(extract_root, DATASET_NAME)\n",
        "\n",
        "if not os.path.isdir(dataset_root):\n",
        "    print(\"Unzipping dataset...\")\n",
        "    !unzip -q \"{zip_path}\" -d \"{extract_root}\"\n",
        "else:\n",
        "    print(\"Dataset folder already exists, skipping unzip.\")\n",
        "\n",
        "print(\"✅ Dataset ready\")\n",
        "print(\"Content of /content:\", os.listdir(\"/content\"))\n",
        "\n",
        "# ==========================================================\n",
        "# 3) Auto-detect train/val/test folders\n",
        "# ==========================================================\n",
        "def find_split_dirs(base_dir):\n",
        "    \"\"\"Find train/val/test under base_dir or any nested folder.\"\"\"\n",
        "    direct_train = os.path.join(base_dir, \"train\")\n",
        "    direct_val   = os.path.join(base_dir, \"val\")\n",
        "    direct_test  = os.path.join(base_dir, \"test\")\n",
        "\n",
        "    if os.path.isdir(direct_train) and os.path.isdir(direct_test):\n",
        "        return direct_train, (direct_val if os.path.isdir(direct_val) else None), direct_test\n",
        "\n",
        "    for root, dirs, _ in os.walk(base_dir):\n",
        "        if \"train\" in dirs and \"test\" in dirs:\n",
        "            train_dir = os.path.join(root, \"train\")\n",
        "            test_dir  = os.path.join(root, \"test\")\n",
        "            val_dir   = os.path.join(root, \"val\") if \"val\" in dirs else None\n",
        "            return train_dir, val_dir, test_dir\n",
        "\n",
        "    raise FileNotFoundError(f\"Could not find train/test folders under: {base_dir}\")\n",
        "\n",
        "TRAIN_DIR, VAL_DIR, TEST_DIR = find_split_dirs(dataset_root)\n",
        "print(\"TRAIN_DIR:\", TRAIN_DIR)\n",
        "print(\"VAL_DIR:\", VAL_DIR)\n",
        "print(\"TEST_DIR:\", TEST_DIR)\n",
        "\n",
        "# If val doesn't exist, create one from train\n",
        "AUTO_SPLIT_VAL_IF_MISSING = True\n",
        "VAL_SPLIT = 0.15  # 15% validation\n",
        "\n",
        "# ==========================================================\n",
        "# 4) Config\n",
        "# ==========================================================\n",
        "NUM_CLASSES = 5\n",
        "IMG_SIZE = 320\n",
        "BATCH_SIZE = 16         # if OOM: 8\n",
        "EPOCHS = 15             # 12–20\n",
        "LR = 1e-4\n",
        "WEIGHT_DECAY = 1e-4\n",
        "\n",
        "WARMUP_EPOCHS = 2\n",
        "MIXUP_ALPHA = 0.2\n",
        "EMA_DECAY = 0.999\n",
        "\n",
        "USE_AMP = True\n",
        "GRAD_ACCUM_STEPS = 1    # if batch small: 2 or 4\n",
        "CHECKPOINT_PATH = \"best_densenet121_320_tricks.pt\"\n",
        "\n",
        "# ==========================================================\n",
        "# 5) Transforms\n",
        "# ==========================================================\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
        "\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=7),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "])\n",
        "\n",
        "eval_tfms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "])\n",
        "\n",
        "# ==========================================================\n",
        "# 6) Datasets\n",
        "# ==========================================================\n",
        "train_full = datasets.ImageFolder(TRAIN_DIR, transform=train_tfms)\n",
        "test_ds    = datasets.ImageFolder(TEST_DIR, transform=eval_tfms)\n",
        "\n",
        "if VAL_DIR is not None:\n",
        "    val_ds = datasets.ImageFolder(VAL_DIR, transform=eval_tfms)\n",
        "    train_ds = train_full\n",
        "else:\n",
        "    if not AUTO_SPLIT_VAL_IF_MISSING:\n",
        "        raise FileNotFoundError(\"VAL_DIR missing and AUTO_SPLIT_VAL_IF_MISSING=False\")\n",
        "\n",
        "    from torch.utils.data import Subset\n",
        "    n = len(train_full)\n",
        "    idx = np.arange(n)\n",
        "    np.random.shuffle(idx)\n",
        "    split = int(n * (1 - VAL_SPLIT))\n",
        "    train_idx, val_idx = idx[:split], idx[split:]\n",
        "\n",
        "    train_ds = Subset(train_full, train_idx)\n",
        "\n",
        "    # For val subset, use eval transforms\n",
        "    train_full_eval = datasets.ImageFolder(TRAIN_DIR, transform=eval_tfms)\n",
        "    val_ds = Subset(train_full_eval, val_idx)\n",
        "\n",
        "print(\"Train size:\", len(train_ds), \"Val size:\", len(val_ds), \"Test size:\", len(test_ds))\n",
        "print(\"Classes:\", train_full.classes)\n",
        "\n",
        "# ==========================================================\n",
        "# 7) WeightedRandomSampler\n",
        "# ==========================================================\n",
        "def get_targets(ds):\n",
        "    # Handle Subset(ImageFolder)\n",
        "    if isinstance(ds, torch.utils.data.Subset):\n",
        "        base = ds.dataset  # ImageFolder\n",
        "        return [base.samples[i][1] for i in ds.indices]\n",
        "    else:\n",
        "        return [y for _, y in ds.samples]\n",
        "\n",
        "targets = get_targets(train_ds)\n",
        "class_counts = np.bincount(targets, minlength=NUM_CLASSES)\n",
        "class_weights = 1.0 / np.maximum(class_counts, 1)\n",
        "sample_weights = [class_weights[y] for y in targets]\n",
        "\n",
        "sampler = WeightedRandomSampler(\n",
        "    weights=torch.DoubleTensor(sample_weights),\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(\"Train class counts:\", class_counts)\n",
        "\n",
        "# ==========================================================\n",
        "# 8) Model: DenseNet121\n",
        "# ==========================================================\n",
        "model = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
        "in_features = model.classifier.in_features\n",
        "model.classifier = nn.Linear(in_features, NUM_CLASSES)\n",
        "model = model.to(device)\n",
        "\n",
        "# ==========================================================\n",
        "# 9) EMA\n",
        "# ==========================================================\n",
        "class EMA:\n",
        "    def __init__(self, model, decay=0.999):\n",
        "        self.decay = decay\n",
        "        self.shadow = {}\n",
        "        for name, p in model.named_parameters():\n",
        "            if p.requires_grad:\n",
        "                self.shadow[name] = p.data.clone()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update(self, model):\n",
        "        for name, p in model.named_parameters():\n",
        "            if p.requires_grad:\n",
        "                self.shadow[name].mul_(self.decay).add_(p.data, alpha=1 - self.decay)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def apply_to(self, model):\n",
        "        self.backup = {}\n",
        "        for name, p in model.named_parameters():\n",
        "            if p.requires_grad:\n",
        "                self.backup[name] = p.data.clone()\n",
        "                p.data.copy_(self.shadow[name])\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def restore(self, model):\n",
        "        for name, p in model.named_parameters():\n",
        "            if p.requires_grad:\n",
        "                p.data.copy_(self.backup[name])\n",
        "        self.backup = None\n",
        "\n",
        "ema = EMA(model, decay=EMA_DECAY)\n",
        "\n",
        "# ==========================================================\n",
        "# 10) MixUp\n",
        "# ==========================================================\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    if alpha <= 0:\n",
        "        return x, y, y, 1.0\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    bs = x.size(0)\n",
        "    idx = torch.randperm(bs).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[idx]\n",
        "    y_a, y_b = y, y[idx]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(crit, pred, y_a, y_b, lam):\n",
        "    return lam * crit(pred, y_a) + (1 - lam) * crit(pred, y_b)\n",
        "\n",
        "# ==========================================================\n",
        "# 11) Loss, Optimizer, Scheduler (Cosine + warmup)\n",
        "# ==========================================================\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "def lr_lambda(epoch):\n",
        "    if epoch < WARMUP_EPOCHS:\n",
        "        return float(epoch + 1) / float(max(1, WARMUP_EPOCHS))\n",
        "    progress = (epoch - WARMUP_EPOCHS) / float(max(1, EPOCHS - WARMUP_EPOCHS))\n",
        "    return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
        "\n",
        "scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(USE_AMP and device.type == \"cuda\"))\n",
        "\n",
        "# ==========================================================\n",
        "# 12) Eval function\n",
        "# ==========================================================\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    all_preds, all_targets = [], []\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        y = y.to(device, non_blocking=True)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "\n",
        "        total_loss += loss.item() * x.size(0)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        all_preds.append(preds.cpu().numpy())\n",
        "        all_targets.append(y.cpu().numpy())\n",
        "\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_targets = np.concatenate(all_targets)\n",
        "\n",
        "    acc = accuracy_score(all_targets, all_preds)\n",
        "    macro_f1 = f1_score(all_targets, all_preds, average=\"macro\")\n",
        "    cm = confusion_matrix(all_targets, all_preds)\n",
        "\n",
        "    return total_loss / len(loader.dataset), acc, macro_f1, cm, all_targets, all_preds\n",
        "\n",
        "# ==========================================================\n",
        "# 13) Training\n",
        "# ==========================================================\n",
        "best_val_f1 = -1.0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    t0 = time.time()\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    for step, (x, y) in enumerate(train_loader):\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        y = y.to(device, non_blocking=True)\n",
        "\n",
        "        if MIXUP_ALPHA > 0:\n",
        "            x, y_a, y_b, lam = mixup_data(x, y, alpha=MIXUP_ALPHA)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
        "            logits = model(x)\n",
        "            if MIXUP_ALPHA > 0:\n",
        "                loss = mixup_criterion(criterion, logits, y_a, y_b, lam)\n",
        "            else:\n",
        "                loss = criterion(logits, y)\n",
        "\n",
        "            loss = loss / GRAD_ACCUM_STEPS\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            ema.update(model)\n",
        "\n",
        "        running_loss += loss.item() * x.size(0) * GRAD_ACCUM_STEPS\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "    # Validate using EMA weights\n",
        "    ema.apply_to(model)\n",
        "    val_loss, val_acc, val_f1, val_cm, _, _ = evaluate(model, val_loader)\n",
        "    ema.restore(model)\n",
        "\n",
        "    dt = time.time() - t0\n",
        "    print(f\"Epoch {epoch+1:02d}/{EPOCHS} | \"\n",
        "          f\"train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | \"\n",
        "          f\"val_acc={val_acc:.4f} | val_macroF1={val_f1:.4f} | \"\n",
        "          f\"lr={scheduler.get_last_lr()[0]:.2e} | time={dt:.1f}s\")\n",
        "\n",
        "    if val_f1 > best_val_f1:\n",
        "        best_val_f1 = val_f1\n",
        "        ckpt = {\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"model\": copy.deepcopy(model.state_dict()),\n",
        "            \"ema_shadow\": copy.deepcopy(ema.shadow),\n",
        "            \"best_val_macro_f1\": best_val_f1,\n",
        "            \"img_size\": IMG_SIZE\n",
        "        }\n",
        "        torch.save(ckpt, CHECKPOINT_PATH)\n",
        "        print(f\"  ✅ Saved best checkpoint: {CHECKPOINT_PATH} (val_macroF1={best_val_f1:.4f})\")\n",
        "\n",
        "print(\"\\n✅ Best Val Macro-F1:\", best_val_f1)\n",
        "\n",
        "# ==========================================================\n",
        "# 14) Test Best Checkpoint (EMA)\n",
        "# ==========================================================\n",
        "ckpt = torch.load(CHECKPOINT_PATH, map_location=\"cpu\")\n",
        "model.load_state_dict(ckpt[\"model\"])\n",
        "model = model.to(device)\n",
        "ema.shadow = ckpt[\"ema_shadow\"]\n",
        "\n",
        "ema.apply_to(model)\n",
        "test_loss, test_acc, test_f1, test_cm, y_true, y_pred = evaluate(model, test_loader)\n",
        "ema.restore(model)\n",
        "\n",
        "print(\"\\n===== TEST RESULTS (Best EMA) =====\")\n",
        "print(f\"Test Loss:     {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Test Macro-F1: {test_f1:.4f}\")\n",
        "print(\"\\nConfusion Matrix:\\n\", test_cm)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, digits=4))\n",
        "\n",
        "# Save plain state dict too\n",
        "torch.save(model.state_dict(), \"best_densenet121_320_tricks_state_dict.pt\")\n",
        "print(\"\\nSaved: best_densenet121_320_tricks_state_dict.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# DenseNet121 @384 with:\n",
        "# - Drive ZIP auto-load + unzip\n",
        "# - auto-detect train/val/test\n",
        "# - WeightedRandomSampler\n",
        "# - MixUp schedule (on first half, off second half)\n",
        "# - EMA\n",
        "# - Cosine LR + warmup\n",
        "# - AMP\n",
        "# - Label Smoothing CE\n",
        "# Target: push test accuracy > 0.70\n",
        "# ============================================\n",
        "\n",
        "# ----------------------------\n",
        "# 0) Imports\n",
        "# ----------------------------\n",
        "import os, math, time, copy, random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "from sklearn.metrics import f1_score, confusion_matrix, classification_report, accuracy_score\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Reproducibility + Device\n",
        "# ----------------------------\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "\n",
        "seed_everything(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ==========================================================\n",
        "# 2) DATASET LOADING FROM GOOGLE DRIVE ZIP\n",
        "# ==========================================================\n",
        "DATASET_NAME = \"knee-osteoarthritis-dataset-with-severity\"  # folder name after unzip\n",
        "ZIP_NAME = \"knee_oa_dataset.zip\"  # <-- CHANGE if your zip name differs\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "zip_path = f\"/content/drive/MyDrive/{ZIP_NAME}\"\n",
        "if not os.path.isfile(zip_path):\n",
        "    raise FileNotFoundError(f\"ZIP file not found at: {zip_path}\\n\"\n",
        "                            f\"➡️ Upload dataset zip to Google Drive > MyDrive and set ZIP_NAME correctly.\")\n",
        "\n",
        "extract_root = \"/content\"\n",
        "dataset_root = os.path.join(extract_root, DATASET_NAME)\n",
        "\n",
        "if not os.path.isdir(dataset_root):\n",
        "    print(\"Unzipping dataset...\")\n",
        "    !unzip -q \"{zip_path}\" -d \"{extract_root}\"\n",
        "else:\n",
        "    print(\"Dataset folder already exists, skipping unzip.\")\n",
        "\n",
        "print(\"✅ Dataset ready\")\n",
        "print(\"Content of /content:\", os.listdir(\"/content\"))\n",
        "\n",
        "# ==========================================================\n",
        "# 3) Auto-detect train/val/test\n",
        "# ==========================================================\n",
        "def find_split_dirs(base_dir):\n",
        "    direct_train = os.path.join(base_dir, \"train\")\n",
        "    direct_val   = os.path.join(base_dir, \"val\")\n",
        "    direct_test  = os.path.join(base_dir, \"test\")\n",
        "\n",
        "    if os.path.isdir(direct_train) and os.path.isdir(direct_test):\n",
        "        return direct_train, (direct_val if os.path.isdir(direct_val) else None), direct_test\n",
        "\n",
        "    for root, dirs, _ in os.walk(base_dir):\n",
        "        if \"train\" in dirs and \"test\" in dirs:\n",
        "            train_dir = os.path.join(root, \"train\")\n",
        "            test_dir  = os.path.join(root, \"test\")\n",
        "            val_dir   = os.path.join(root, \"val\") if \"val\" in dirs else None\n",
        "            return train_dir, val_dir, test_dir\n",
        "\n",
        "    raise FileNotFoundError(f\"Could not find train/test folders under: {base_dir}\")\n",
        "\n",
        "TRAIN_DIR, VAL_DIR, TEST_DIR = find_split_dirs(dataset_root)\n",
        "print(\"TRAIN_DIR:\", TRAIN_DIR)\n",
        "print(\"VAL_DIR:\", VAL_DIR)\n",
        "print(\"TEST_DIR:\", TEST_DIR)\n",
        "\n",
        "AUTO_SPLIT_VAL_IF_MISSING = True\n",
        "VAL_SPLIT = 0.15\n",
        "\n",
        "# ==========================================================\n",
        "# 4) Config (UPDATED)\n",
        "# ==========================================================\n",
        "NUM_CLASSES = 5\n",
        "\n",
        "IMG_SIZE = 384          # ✅ increased from 320\n",
        "BATCH_SIZE = 8          # ✅ reduce for T4 safety at 384\n",
        "EPOCHS = 18             # ✅ slightly longer\n",
        "\n",
        "LR = 1e-4\n",
        "WEIGHT_DECAY = 1e-4\n",
        "\n",
        "WARMUP_EPOCHS = 2\n",
        "MIXUP_ALPHA = 0.2       # base mixup alpha for first half (schedule below)\n",
        "EMA_DECAY = 0.999\n",
        "\n",
        "USE_AMP = True\n",
        "GRAD_ACCUM_STEPS = 1    # if you want effective batch 16, set 2\n",
        "CHECKPOINT_PATH = \"best_densenet121_384_ls_mixupSchedule_ema.pt\"\n",
        "\n",
        "LABEL_SMOOTHING = 0.07  # ✅ helps KL1/adjacent confusion\n",
        "\n",
        "# ==========================================================\n",
        "# 5) Transforms\n",
        "# ==========================================================\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
        "\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=7),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "])\n",
        "\n",
        "eval_tfms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "])\n",
        "\n",
        "# ==========================================================\n",
        "# 6) Datasets\n",
        "# ==========================================================\n",
        "train_full = datasets.ImageFolder(TRAIN_DIR, transform=train_tfms)\n",
        "test_ds    = datasets.ImageFolder(TEST_DIR, transform=eval_tfms)\n",
        "\n",
        "if VAL_DIR is not None:\n",
        "    val_ds = datasets.ImageFolder(VAL_DIR, transform=eval_tfms)\n",
        "    train_ds = train_full\n",
        "else:\n",
        "    if not AUTO_SPLIT_VAL_IF_MISSING:\n",
        "        raise FileNotFoundError(\"VAL_DIR missing and AUTO_SPLIT_VAL_IF_MISSING=False\")\n",
        "    from torch.utils.data import Subset\n",
        "    n = len(train_full)\n",
        "    idx = np.arange(n)\n",
        "    np.random.shuffle(idx)\n",
        "    split = int(n * (1 - VAL_SPLIT))\n",
        "    train_idx, val_idx = idx[:split], idx[split:]\n",
        "    train_ds = Subset(train_full, train_idx)\n",
        "\n",
        "    train_full_eval = datasets.ImageFolder(TRAIN_DIR, transform=eval_tfms)\n",
        "    val_ds = Subset(train_full_eval, val_idx)\n",
        "\n",
        "print(\"Train size:\", len(train_ds), \"Val size:\", len(val_ds), \"Test size:\", len(test_ds))\n",
        "print(\"Classes:\", train_full.classes)\n",
        "\n",
        "# ==========================================================\n",
        "# 7) WeightedRandomSampler\n",
        "# ==========================================================\n",
        "def get_targets(ds):\n",
        "    if isinstance(ds, torch.utils.data.Subset):\n",
        "        base = ds.dataset\n",
        "        return [base.samples[i][1] for i in ds.indices]\n",
        "    return [y for _, y in ds.samples]\n",
        "\n",
        "targets = get_targets(train_ds)\n",
        "class_counts = np.bincount(targets, minlength=NUM_CLASSES)\n",
        "class_weights = 1.0 / np.maximum(class_counts, 1)\n",
        "sample_weights = [class_weights[y] for y in targets]\n",
        "\n",
        "sampler = WeightedRandomSampler(\n",
        "    weights=torch.DoubleTensor(sample_weights),\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(\"Train class counts:\", class_counts)\n",
        "\n",
        "# ==========================================================\n",
        "# 8) Model: DenseNet121\n",
        "# ==========================================================\n",
        "model = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
        "in_features = model.classifier.in_features\n",
        "model.classifier = nn.Linear(in_features, NUM_CLASSES)\n",
        "model = model.to(device)\n",
        "\n",
        "# ==========================================================\n",
        "# 9) EMA\n",
        "# ==========================================================\n",
        "class EMA:\n",
        "    def __init__(self, model, decay=0.999):\n",
        "        self.decay = decay\n",
        "        self.shadow = {}\n",
        "        for name, p in model.named_parameters():\n",
        "            if p.requires_grad:\n",
        "                self.shadow[name] = p.data.clone()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update(self, model):\n",
        "        for name, p in model.named_parameters():\n",
        "            if p.requires_grad:\n",
        "                self.shadow[name].mul_(self.decay).add_(p.data, alpha=1 - self.decay)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def apply_to(self, model):\n",
        "        self.backup = {}\n",
        "        for name, p in model.named_parameters():\n",
        "            if p.requires_grad:\n",
        "                self.backup[name] = p.data.clone()\n",
        "                p.data.copy_(self.shadow[name])\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def restore(self, model):\n",
        "        for name, p in model.named_parameters():\n",
        "            if p.requires_grad:\n",
        "                p.data.copy_(self.backup[name])\n",
        "        self.backup = None\n",
        "\n",
        "ema = EMA(model, decay=EMA_DECAY)\n",
        "\n",
        "# ==========================================================\n",
        "# 10) MixUp\n",
        "# ==========================================================\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    if alpha <= 0:\n",
        "        return x, y, y, 1.0\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    bs = x.size(0)\n",
        "    idx = torch.randperm(bs).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[idx]\n",
        "    y_a, y_b = y, y[idx]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(crit, pred, y_a, y_b, lam):\n",
        "    return lam * crit(pred, y_a) + (1 - lam) * crit(pred, y_b)\n",
        "\n",
        "# ==========================================================\n",
        "# 11) Loss (UPDATED: label smoothing), Optimizer, Scheduler\n",
        "# ==========================================================\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "def lr_lambda(epoch):\n",
        "    if epoch < WARMUP_EPOCHS:\n",
        "        return float(epoch + 1) / float(max(1, WARMUP_EPOCHS))\n",
        "    progress = (epoch - WARMUP_EPOCHS) / float(max(1, EPOCHS - WARMUP_EPOCHS))\n",
        "    return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
        "\n",
        "scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(USE_AMP and device.type == \"cuda\"))\n",
        "\n",
        "# ==========================================================\n",
        "# 12) Eval\n",
        "# ==========================================================\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    all_preds, all_targets = [], []\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        y = y.to(device, non_blocking=True)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "\n",
        "        total_loss += loss.item() * x.size(0)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        all_preds.append(preds.cpu().numpy())\n",
        "        all_targets.append(y.cpu().numpy())\n",
        "\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_targets = np.concatenate(all_targets)\n",
        "\n",
        "    acc = accuracy_score(all_targets, all_preds)\n",
        "    macro_f1 = f1_score(all_targets, all_preds, average=\"macro\")\n",
        "    cm = confusion_matrix(all_targets, all_preds)\n",
        "\n",
        "    return total_loss / len(loader.dataset), acc, macro_f1, cm, all_targets, all_preds\n",
        "\n",
        "# ==========================================================\n",
        "# 13) Train (UPDATED: MixUp schedule)\n",
        "# ==========================================================\n",
        "best_val_f1 = -1.0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    t0 = time.time()\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # ✅ MixUp schedule: ON in first half, OFF in second half\n",
        "    current_mixup = MIXUP_ALPHA if epoch < (EPOCHS // 2) else 0.0\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    for step, (x, y) in enumerate(train_loader):\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        y = y.to(device, non_blocking=True)\n",
        "\n",
        "        if current_mixup > 0:\n",
        "            x, y_a, y_b, lam = mixup_data(x, y, alpha=current_mixup)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
        "            logits = model(x)\n",
        "            if current_mixup > 0:\n",
        "                loss = mixup_criterion(criterion, logits, y_a, y_b, lam)\n",
        "            else:\n",
        "                loss = criterion(logits, y)\n",
        "\n",
        "            loss = loss / GRAD_ACCUM_STEPS\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            ema.update(model)\n",
        "\n",
        "        running_loss += loss.item() * x.size(0) * GRAD_ACCUM_STEPS\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "    # Validate using EMA weights\n",
        "    ema.apply_to(model)\n",
        "    val_loss, val_acc, val_f1, val_cm, _, _ = evaluate(model, val_loader)\n",
        "    ema.restore(model)\n",
        "\n",
        "    dt = time.time() - t0\n",
        "    print(f\"Epoch {epoch+1:02d}/{EPOCHS} | \"\n",
        "          f\"mixup={current_mixup:.2f} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | \"\n",
        "          f\"val_acc={val_acc:.4f} | val_macroF1={val_f1:.4f} | \"\n",
        "          f\"lr={scheduler.get_last_lr()[0]:.2e} | time={dt:.1f}s\")\n",
        "\n",
        "    if val_f1 > best_val_f1:\n",
        "        best_val_f1 = val_f1\n",
        "        ckpt = {\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"model\": copy.deepcopy(model.state_dict()),\n",
        "            \"ema_shadow\": copy.deepcopy(ema.shadow),\n",
        "            \"best_val_macro_f1\": best_val_f1,\n",
        "            \"img_size\": IMG_SIZE,\n",
        "            \"label_smoothing\": LABEL_SMOOTHING,\n",
        "            \"mixup_schedule\": \"first_half_on_second_half_off\"\n",
        "        }\n",
        "        torch.save(ckpt, CHECKPOINT_PATH)\n",
        "        print(f\"  ✅ Saved best checkpoint: {CHECKPOINT_PATH} (val_macroF1={best_val_f1:.4f})\")\n",
        "\n",
        "print(\"\\n✅ Best Val Macro-F1:\", best_val_f1)\n",
        "\n",
        "# ==========================================================\n",
        "# 14) Test best checkpoint (EMA)\n",
        "# ==========================================================\n",
        "ckpt = torch.load(CHECKPOINT_PATH, map_location=\"cpu\")\n",
        "model.load_state_dict(ckpt[\"model\"])\n",
        "model = model.to(device)\n",
        "ema.shadow = ckpt[\"ema_shadow\"]\n",
        "\n",
        "ema.apply_to(model)\n",
        "test_loss, test_acc, test_f1, test_cm, y_true, y_pred = evaluate(model, test_loader)\n",
        "ema.restore(model)\n",
        "\n",
        "print(\"\\n===== TEST RESULTS (Best EMA) =====\")\n",
        "print(f\"Test Loss:     {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Test Macro-F1: {test_f1:.4f}\")\n",
        "print(\"\\nConfusion Matrix:\\n\", test_cm)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, digits=4))\n",
        "\n",
        "torch.save(model.state_dict(), \"best_densenet121_384_final_state_dict.pt\")\n",
        "print(\"\\nSaved: best_densenet121_384_final_state_dict.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qP9u0hFUXlkA",
        "outputId": "bb918a92-a623-4a69-9f51-f55c3d9d1aec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Dataset folder already exists, skipping unzip.\n",
            "✅ Dataset ready\n",
            "Content of /content: ['.config', 'knee-osteoarthritis-dataset-with-severity', 'best_densenet121_320_tricks.pt', 'drive', 'best_densenet121_320_tricks_state_dict.pt', 'sample_data']\n",
            "TRAIN_DIR: /content/knee-osteoarthritis-dataset-with-severity/train\n",
            "VAL_DIR: /content/knee-osteoarthritis-dataset-with-severity/val\n",
            "TEST_DIR: /content/knee-osteoarthritis-dataset-with-severity/test\n",
            "Train size: 5778 Val size: 826 Test size: 1656\n",
            "Classes: ['0', '1', '2', '3', '4']\n",
            "Train class counts: [2286 1046 1516  757  173]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-777/155698652.py:267: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(USE_AMP and device.type == \"cuda\"))\n",
            "/tmp/ipython-input-777/155698652.py:323: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
            "/tmp/ipython-input-777/155698652.py:282: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01/18 | mixup=0.20 | train_loss=1.2674 | val_loss=1.7932 | val_acc=0.0327 | val_macroF1=0.0127 | lr=1.00e-04 | time=135.4s\n",
            "  ✅ Saved best checkpoint: best_densenet121_384_ls_mixupSchedule_ema.pt (val_macroF1=0.0127)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-777/155698652.py:323: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
            "/tmp/ipython-input-777/155698652.py:282: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 02/18 | mixup=0.20 | train_loss=1.1291 | val_loss=1.7927 | val_acc=0.0363 | val_macroF1=0.0163 | lr=1.00e-04 | time=115.5s\n",
            "  ✅ Saved best checkpoint: best_densenet121_384_ls_mixupSchedule_ema.pt (val_macroF1=0.0163)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-777/155698652.py:323: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
            "/tmp/ipython-input-777/155698652.py:282: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 03/18 | mixup=0.20 | train_loss=1.0732 | val_loss=1.2652 | val_acc=0.5847 | val_macroF1=0.4662 | lr=9.90e-05 | time=115.7s\n",
            "  ✅ Saved best checkpoint: best_densenet121_384_ls_mixupSchedule_ema.pt (val_macroF1=0.4662)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-777/155698652.py:323: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
            "/tmp/ipython-input-777/155698652.py:282: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 04/18 | mixup=0.20 | train_loss=1.0169 | val_loss=1.1503 | val_acc=0.5823 | val_macroF1=0.5409 | lr=9.62e-05 | time=115.6s\n",
            "  ✅ Saved best checkpoint: best_densenet121_384_ls_mixupSchedule_ema.pt (val_macroF1=0.5409)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-777/155698652.py:323: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
            "/tmp/ipython-input-777/155698652.py:282: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 05/18 | mixup=0.20 | train_loss=0.9850 | val_loss=1.0465 | val_acc=0.5993 | val_macroF1=0.6327 | lr=9.16e-05 | time=116.3s\n",
            "  ✅ Saved best checkpoint: best_densenet121_384_ls_mixupSchedule_ema.pt (val_macroF1=0.6327)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-777/155698652.py:323: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
            "/tmp/ipython-input-777/155698652.py:282: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 06/18 | mixup=0.20 | train_loss=0.9627 | val_loss=0.9316 | val_acc=0.6840 | val_macroF1=0.6928 | lr=8.54e-05 | time=117.3s\n",
            "  ✅ Saved best checkpoint: best_densenet121_384_ls_mixupSchedule_ema.pt (val_macroF1=0.6928)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-777/155698652.py:323: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
            "/tmp/ipython-input-777/155698652.py:282: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 07/18 | mixup=0.20 | train_loss=0.9664 | val_loss=0.9094 | val_acc=0.6864 | val_macroF1=0.6813 | lr=7.78e-05 | time=119.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-777/155698652.py:323: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
            "/tmp/ipython-input-777/155698652.py:282: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 08/18 | mixup=0.20 | train_loss=0.9234 | val_loss=0.8982 | val_acc=0.6889 | val_macroF1=0.6975 | lr=6.91e-05 | time=116.9s\n",
            "  ✅ Saved best checkpoint: best_densenet121_384_ls_mixupSchedule_ema.pt (val_macroF1=0.6975)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-777/155698652.py:323: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
            "/tmp/ipython-input-777/155698652.py:282: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 09/18 | mixup=0.20 | train_loss=0.8900 | val_loss=0.8980 | val_acc=0.6973 | val_macroF1=0.7038 | lr=5.98e-05 | time=119.7s\n",
            "  ✅ Saved best checkpoint: best_densenet121_384_ls_mixupSchedule_ema.pt (val_macroF1=0.7038)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-777/155698652.py:323: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
            "/tmp/ipython-input-777/155698652.py:282: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/18 | mixup=0.00 | train_loss=0.6770 | val_loss=0.9464 | val_acc=0.6755 | val_macroF1=0.6771 | lr=5.00e-05 | time=117.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-777/155698652.py:323: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
            "/tmp/ipython-input-777/155698652.py:282: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/18 | mixup=0.00 | train_loss=0.6436 | val_loss=0.9557 | val_acc=0.6743 | val_macroF1=0.6948 | lr=4.02e-05 | time=116.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-777/155698652.py:323: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
            "/tmp/ipython-input-777/155698652.py:282: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/18 | mixup=0.00 | train_loss=0.5899 | val_loss=1.0950 | val_acc=0.6199 | val_macroF1=0.6665 | lr=3.09e-05 | time=115.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-777/155698652.py:323: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
            "/tmp/ipython-input-777/155698652.py:282: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/18 | mixup=0.00 | train_loss=0.5362 | val_loss=1.0188 | val_acc=0.6707 | val_macroF1=0.6731 | lr=2.22e-05 | time=116.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-777/155698652.py:323: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
            "/tmp/ipython-input-777/155698652.py:282: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/18 | mixup=0.00 | train_loss=0.5030 | val_loss=1.0540 | val_acc=0.6634 | val_macroF1=0.6820 | lr=1.46e-05 | time=116.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-777/155698652.py:323: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
            "/tmp/ipython-input-777/155698652.py:282: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/18 | mixup=0.00 | train_loss=0.4791 | val_loss=1.1246 | val_acc=0.6513 | val_macroF1=0.6670 | lr=8.43e-06 | time=115.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-777/155698652.py:323: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
            "/tmp/ipython-input-777/155698652.py:282: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/18 | mixup=0.00 | train_loss=0.4636 | val_loss=1.1084 | val_acc=0.6513 | val_macroF1=0.6719 | lr=3.81e-06 | time=116.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-777/155698652.py:323: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
            "/tmp/ipython-input-777/155698652.py:282: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/18 | mixup=0.00 | train_loss=0.4483 | val_loss=1.1050 | val_acc=0.6562 | val_macroF1=0.6824 | lr=9.61e-07 | time=116.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-777/155698652.py:323: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n",
            "/tmp/ipython-input-777/155698652.py:282: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/18 | mixup=0.00 | train_loss=0.4352 | val_loss=1.1069 | val_acc=0.6465 | val_macroF1=0.6646 | lr=0.00e+00 | time=116.7s\n",
            "\n",
            "✅ Best Val Macro-F1: 0.7037964064015501\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-777/155698652.py:282: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device.type == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== TEST RESULTS (Best EMA) =====\n",
            "Test Loss:     0.8544\n",
            "Test Accuracy: 0.7089\n",
            "Test Macro-F1: 0.7085\n",
            "\n",
            "Confusion Matrix:\n",
            " [[547  73  18   1   0]\n",
            " [130 102  59   5   0]\n",
            " [ 38  83 299  27   0]\n",
            " [  0   4  28 181  10]\n",
            " [  0   0   0   6  45]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7650    0.8560    0.8080       639\n",
            "           1     0.3893    0.3446    0.3656       296\n",
            "           2     0.7401    0.6689    0.7027       447\n",
            "           3     0.8227    0.8117    0.8172       223\n",
            "           4     0.8182    0.8824    0.8491        51\n",
            "\n",
            "    accuracy                         0.7089      1656\n",
            "   macro avg     0.7071    0.7127    0.7085      1656\n",
            "weighted avg     0.7006    0.7089    0.7030      1656\n",
            "\n",
            "\n",
            "Saved: best_densenet121_384_final_state_dict.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Setting         | DenseNet121 @320 | DenseNet121 @384 (Final) |\n",
        "| --------------- | ---------------- | ------------------------ |\n",
        "| Image Size      | 320×320          | 384×384                  |\n",
        "| Batch Size      | 16               | 8                        |\n",
        "| Epochs          | 15               | 18                       |\n",
        "| Label Smoothing | ❌                | ✅ (0.07)                 |\n",
        "| MixUp           | Always ON        | Scheduled (ON → OFF)     |\n",
        "| EMA             | ✅                | ✅                        |\n",
        "| Test Accuracy   | 0.6739           | **0.7089**               |\n",
        "| Test Macro-F1   | 0.6914           | **0.7085**               |\n"
      ],
      "metadata": {
        "id": "YmfOiefkU0Iy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The DenseNet121 @384 configuration is selected as the final best-performing single-model pipeline for submission due to its superior test accuracy (70.89%) and macro-F1 (70.85%)."
      ],
      "metadata": {
        "id": "V3bxACbuU93M"
      }
    }
  ]
}